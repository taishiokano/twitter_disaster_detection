{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 05. Training Embedding Vector from Scratch\n",
    "In this module, rather than using pre-trained embedding vector, we allow an embedding vector changed as same as other parameters in model. In other words, An embedding vector can be trained along with the rest of the parameters."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialized Data Set\n",
    "As usual, import our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sentence for Example 0 ---\n",
      "deed reason earthquake may allah forgive u\n",
      "The label for Example 0 ---\n",
      "1\n",
      "\n",
      "The sentence for Example 1 ---\n",
      "forest fire near la ronge sask canada\n",
      "The label for Example 1 ---\n",
      "1\n",
      "\n",
      "The sentence for Example 2 ---\n",
      "resident asked place notified officer evacuation shelter place order expected\n",
      "The label for Example 2 ---\n",
      "1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# import data set\n",
    "import pandas as pd\n",
    "df = pd.read_csv(\"data/cleaned-train-tweets.csv\", sep=\"|\")\n",
    "\n",
    "# create PyTorch data set\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "class DisasterTweetsDataset(Dataset):\n",
    "    def __init__(self, df, max_vocab_size, vocabulary = None):\n",
    "        \n",
    "        # load dataframe\n",
    "        self.x = df[\"clean_text\"]\n",
    "        self.y = df[\"target\"]\n",
    "\n",
    "        # create vocabulary\n",
    "        if not vocabulary:\n",
    "            self.vocab = build_vocab_from_iterator(\n",
    "                [\" \".join([str(text) for text in df[\"clean_text\"]]).split()],\n",
    "                specials=['<unk>'],\n",
    "                max_tokens = max_vocab_size)\n",
    "            self.vocab.set_default_index(self.vocab['<unk>'])\n",
    "        else:\n",
    "            self.vocab = vocabulary\n",
    "\n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        # add one more for out of vocab words\n",
    "        return len(self.vocab) + 1\n",
    "    \n",
    "    def get_vocab(self):\n",
    "        return self.vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        x = self.x[i]\n",
    "        y = self.y[i]\n",
    "        return (y, x)\n",
    "\n",
    "# initiate dataset and vocabulary\n",
    "dataset = DisasterTweetsDataset(df, 10000)\n",
    "vocab = dataset.get_vocab()\n",
    "\n",
    "for i, (label, tweet) in enumerate(dataset):\n",
    "    print(f'The sentence for Example {i} ---')\n",
    "    print(tweet)\n",
    "    print(f'The label for Example {i} ---')\n",
    "    print(label)\n",
    "    print()\n",
    "    if i == 2: break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Embedding Vector the Collation Function\n",
    "Now, we have to define the way that we can encode each token into a number. Initially, we should define the initial embedding matrix that have the number of embeddings equals to the vocabulary size and the embedding dimension equal to 300 (our presumed number to resembles other pre-trained embedding matrix). For simplicity, we start with collating our tweet in bag of words. Hence, each tweet will transform to a vector with the dimension equals to the vocabulary size (10,000) and the value for each feature is equal to the frequency of each token in each tweet.\n",
    "\n",
    "Note that this embedding vector is just the initial vector. In training pipeline, we will also train this embedding matrix along with other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_into_bow(batch):\n",
    "    \"\"\"\n",
    "    collate the dataset into bag of words representation\n",
    "    \n",
    "    input:\n",
    "        - batch (list(str, int)): a list of dataset in forms of label, text\n",
    "    return:\n",
    "        - (tensor): a tensor of labels\n",
    "        - (tensor): a tensor of bag of words\n",
    "    \"\"\"\n",
    "    def text_pipeline(text):\n",
    "        \"\"\"\n",
    "        create bow vector for each text\n",
    "        \n",
    "        input:\n",
    "            - text (str): a document, a text\n",
    "        return:\n",
    "            - (list): a bow vector\n",
    "        \"\"\"\n",
    "\n",
    "        indices = vocab(str(text).split())\n",
    "        bows = [0] * len(vocab)\n",
    "        for index in indices:\n",
    "            bows[index] += 1 / len(indices)\n",
    "        return bows\n",
    "\n",
    "    labels, texts = [], []\n",
    "    for label, text in batch:\n",
    "        labels.append(int(label))\n",
    "        texts.append(text_pipeline(text))\n",
    "    return torch.tensor(labels, dtype=torch.int64), torch.tensor(texts)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Pipeline\n",
    "Now, we will training our model using bidirectional long-short-term memory RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate a dataloader and split the data into train and validation dataset\n",
    "from torch.utils.data.dataset import random_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def new_dataloader(dataset, collate_fn, batch_size=64, split_train_ratio=0.7):\n",
    "    num_train = int(len(df) * split_train_ratio)\n",
    "    num_valid = len(df) - num_train\n",
    "    train_data, valid_data = random_split(\n",
    "        dataset,\n",
    "        [num_train, num_valid]\n",
    "    )\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data, \n",
    "        batch_size=batch_size, \n",
    "        shuffle=True,\n",
    "        collate_fn=collate_fn\n",
    "    )\n",
    "    valid_dataloader = DataLoader(\n",
    "        valid_data, \n",
    "        batch_size=batch_size,\n",
    "        shuffle=False, \n",
    "        collate_fn=collate_fn)\n",
    "    return (train_dataloader, valid_dataloader)\n",
    "\n",
    "# helper function: repackage hidden\n",
    "def repackage_hidden(h):\n",
    "    \"\"\"\n",
    "    Wraps hidden states in new Tensors, to detach them from their history.\n",
    "    \"\"\"\n",
    "    if h is None:\n",
    "        return None\n",
    "    elif isinstance(h, torch.Tensor):\n",
    "        return h.detach()\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v) for v in h)\n",
    "\n",
    "\n",
    "# function for training an epoch\n",
    "def train_an_epoch(dataloader, model, hidden, loss_function, optimizer, \n",
    "                   clip_grad, max_norm):\n",
    "    model.train() # Sets the module in training mode.\n",
    "    log_interval = 500\n",
    "\n",
    "    for idx, (label, text) in enumerate(dataloader):\n",
    "        hidden = repackage_hidden(hidden)\n",
    "        model.zero_grad()\n",
    "        output, hidden = model(text, hidden)\n",
    "        loss = loss_function(output.view(-1, output.size(-1)), label.view(-1))\n",
    "        loss.backward()\n",
    "        if clip_grad:\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                parameters=model.parameters(), \n",
    "                max_norm=max_norm # default GRAD_CLIP = 1\n",
    "            )\n",
    "        optimizer.step()\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            print(f'At iteration {idx} the loss is {loss:.3f}.')\n",
    "\n",
    "# function for calculate the accuracy for a given dataloader\n",
    "def get_accuracy(dataloader, model):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden = None\n",
    "        total_acc, total_count = 0, 0\n",
    "        for _, (label, text) in enumerate(dataloader):\n",
    "            log_probs, hidden = model(text, hidden)\n",
    "            predicted_label = torch.argmax(log_probs, dim=1)\n",
    "            total_acc += (predicted_label == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count\n",
    "\n",
    "# putting all together, create function for training\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "def training(dataset, collate_fn, batch_size, split_train_ratio, \n",
    "             model, epochs, loss_function, optimizer, clip_grad, max_norm):\n",
    "    \n",
    "    # create dataloader from dataset\n",
    "    train_dataloader, valid_dataloader = new_dataloader(\n",
    "        dataset, collate_fn, batch_size, split_train_ratio)\n",
    "\n",
    "    # training\n",
    "    accuracies = []\n",
    "    max_val_acc = -float(\"inf\")\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        hidden = None\n",
    "        train_an_epoch(train_dataloader, model, hidden, loss_function, optimizer,\n",
    "                       clip_grad, max_norm)\n",
    "        accuracy = get_accuracy(valid_dataloader, model)\n",
    "        accuracies.append(accuracy)\n",
    "        time_taken = time.time() - epoch_start_time\n",
    "        if epoch % 10 == 0:\n",
    "            print(f'After epoch {epoch} the validation accuracy is {accuracy:.3f}.')\n",
    "        # persist the best model\n",
    "        if accuracy > max_val_acc:\n",
    "            print(\"the best model has validation accuracy at {}\".format(accuracy))\n",
    "            best_model = type(model)(\n",
    "                model.rnn_type,  \n",
    "                model.input_size, \n",
    "                model.hidden_size, \n",
    "                model.num_labels,\n",
    "                model.num_layers, \n",
    "                model.dropout\n",
    "            )\n",
    "            best_model.load_state_dict(model.state_dict())\n",
    "            max_val_acc = accuracy\n",
    "    \n",
    "    plt.plot(range(1, epochs + 1), accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate an RNN classifier\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    \"\"\"\n",
    "    Initialize RNN classifier\n",
    "\n",
    "    Args:\n",
    "        - rnn_type (str): \"LSTM\", \"BiLSTM\", \"GRU\", \"RNN_TANH\", \"RNN_RELU\"\n",
    "        - input_size (int): size of embedding vector (number of features) \n",
    "            for each word (default: 300)\n",
    "        - hidden_size (int): the number of features in the hidden state (def: 300)\n",
    "    \"\"\"\n",
    "    def __init__(self, input_size, hidden_size, num_labels, \n",
    "                 num_layers, dropout=0.5):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.rnn_type = rnn_type\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_labels = num_labels\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.drop = nn.Dropout(dropout)\n",
    "\n",
    "        self.encoder = nn.Embedding(\n",
    "            num_embeddings=vocab_size, \n",
    "            embedding_dim=embedding_dim)\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.decoder = nn.Linear(\n",
    "            in_features=hidden_size * 2, \n",
    "            out_features=num_labels\n",
    "        )\n",
    "        \n",
    "    \n",
    "    def forward(self, input, hidden0):\n",
    "        embedding = self.drop(self.encoder(input))\n",
    "        output, hidden = self.rnn(embedding, hidden0)\n",
    "        output = self.drop(output)\n",
    "        decoded_output = self.decoder(output)\n",
    "        decoded_output = F.log_softmax(self.decoder(output), dim=1)\n",
    "        return decoded_output, hidden"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing \n",
    "To allow the embedding vector be changed. First, we use only \"text\" data and forget the label data now. For each text row, we compare the centered word with their neighborhood in one window ($W$ = 1). For example, in tweet, `forest fire near la ronge sask canada`, we should generate our new dataset like this: `[fire, near], [fire, forest], [canada, sask], [canada, </s>]` Note that we must add special dummy word `<s>` and `</s>` for taking care the starting and ending. \n",
    "\n",
    "The code below processes and creates new dataset for this task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting only text data\n",
    "W = 1\n",
    "WINDOW_SIZE = (2 * W + 1)\n",
    "\n",
    "SENT_START_WORD = '<s>'\n",
    "SENT_END_WORD = '</s>'\n",
    "SENT_START_TAG = '<STAG>'\n",
    "SENT_END_TAG = '<ETAG>'\n",
    "\n",
    "\n",
    "def add_sent_start_end(data_iter, w):\n",
    "    for (label, tweet) in data_iter:\n",
    "        new_tweet = [SENT_START_WORD] * w + tweet + [SENT_END_WORD] * w\n",
    "        new_label = [SENT_START_TAG] * w + ud_tags + [SENT_END_TAG] * w\n",
    "        ## MISSING PART: ADD YOUR CODE BELOW\n",
    "        new_ptb_tags = [SENT_START_TAG] * w + ptb_tags + [SENT_END_TAG] * w\n",
    "        ## ADD YOUR CODE ABOVE\n",
    "        yield(new_words, new_ud_tags, new_ptb_tags)\n",
    "\n",
    "def create_windows(data_iter, w):\n",
    "    window_size = 2*w + 1\n",
    "    for (words, ud_tags, ptb_tags) in data_iter:\n",
    "        words_zip = zip(*[words[i:] for i in range(window_size)])\n",
    "        ud_zip = zip(*[ud_tags[i:] for i in range(window_size)])\n",
    "        ## MISSING PART: ADD YOUR CODE BELOW\n",
    "        ptb_zip = zip(*[ptb_tags[i:] for i in range(window_size)])\n",
    "        ## ADD YOUR CODE ABOVE\n",
    "        for word_sseq, ud_sseq, ptb_sseq in zip(\n",
    "                words_zip, ud_zip, ptb_zip):\n",
    "            yield(word_sseq, ud_sseq, ptb_sseq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter_vocab = preprocess_data_seq(train_iter_0, 1)\n",
    "\n",
    "counter_words = Counter()\n",
    "counter_ud = Counter()\n",
    "counter_ptb = Counter()\n",
    "for (text, pos_ud, pos_ptb) in train_iter_vocab:\n",
    "    counter_words.update(text)\n",
    "    counter_ud.update(pos_ud)\n",
    "    counter_ptb.update(pos_ptb)\n",
    "\n",
    "\n",
    "vocab_words = torchtext.vocab.vocab(counter_words,  specials = ['<unk>'], \n",
    "                    special_first = True)    \n",
    "vocab_words.set_default_index(0)\n",
    "vocab_ud = torchtext.vocab.vocab(counter_ud)\n",
    "vocab_ptb = torchtext.vocab.vocab(counter_ptb)\n",
    "\n",
    "print(f\"{len(vocab_words)} words, {len(vocab_ud)} ud pos classes, {len(vocab_ptb)} ptb pos classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = 1\n",
    "WINDOW_SIZE = (2 * W + 1)\n",
    "\n",
    "SENT_START_WORD = '<s>'\n",
    "SENT_END_WORD = '</s>'\n",
    "SENT_START_TAG = '<STAG>'\n",
    "SENT_END_TAG = '<ETAG>'\n",
    "\n",
    "\n",
    "def add_sent_start_end(data_iter, w):\n",
    "    for (words, ud_tags, ptb_tags) in data_iter:\n",
    "        new_words = [SENT_START_WORD] * w + words + [SENT_END_WORD] * w\n",
    "        new_ud_tags = [SENT_START_TAG] * w + ud_tags + [SENT_END_TAG] * w\n",
    "        ## MISSING PART: ADD YOUR CODE BELOW\n",
    "        new_ptb_tags = [SENT_START_TAG] * w + ptb_tags + [SENT_END_TAG] * w\n",
    "        ## ADD YOUR CODE ABOVE\n",
    "        yield(new_words, new_ud_tags, new_ptb_tags)\n",
    "        \n",
    "def create_windows(data_iter, w):\n",
    "    window_size = 2*w + 1\n",
    "    for (words, ud_tags, ptb_tags) in data_iter:\n",
    "        words_zip = zip(*[words[i:] for i in range(window_size)])\n",
    "        ud_zip = zip(*[ud_tags[i:] for i in range(window_size)])\n",
    "        ## MISSING PART: ADD YOUR CODE BELOW\n",
    "        ptb_zip = zip(*[ptb_tags[i:] for i in range(window_size)])\n",
    "        ## ADD YOUR CODE ABOVE\n",
    "        for word_sseq, ud_sseq, ptb_sseq in zip(\n",
    "                words_zip, ud_zip, ptb_zip):\n",
    "            yield(word_sseq, ud_sseq, ptb_sseq)\n",
    "            \n",
    "def preprocess_data_seq(data_iter, w):\n",
    "    ## MISSING PART: ADD YOUR CODE BELOW\n",
    "    return create_windows(add_sent_start_end(data_iter, w), w)\n",
    "\n",
    "\n",
    "def test_preprocess_data_seq():\n",
    "    \n",
    "    # WARNING: The following test assumes a particular default\n",
    "    # sequence of examples in the PyTorch UDPOS dataset. If you\n",
    "    # suspect the sequence is different for your dataset, please\n",
    "    # adapt the test.\n",
    "\n",
    "    train_iter_0 = torchtext.datasets.UDPOS(split = 'train')    \n",
    "    train_iter_demo = preprocess_data_seq(train_iter_0, 1)\n",
    "    ex0 = (('<s>', 'Al', '-'), \n",
    "           ('<STAG>', 'PROPN', 'PUNCT'), \n",
    "           ('<STAG>', 'NNP', 'HYPH'))\n",
    "    ex1 = (('Al', '-', 'Zaman'), \n",
    "           ('PROPN', 'PUNCT', 'PROPN'), \n",
    "           ('NNP', 'HYPH', 'NNP'))\n",
    "    ex2 = (('-', 'Zaman', ':'), \n",
    "           ('PUNCT', 'PROPN', 'PUNCT'), \n",
    "           ('HYPH', 'NNP', ':'))\n",
    "    assert ex0 == next(train_iter_demo)\n",
    "    assert ex1 == next(train_iter_demo)\n",
    "    assert ex2 == next(train_iter_demo)\n",
    "    print('Test passed.')\n",
    "\n",
    "\n",
    "test_preprocess_data_seq()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Vocab()"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a Word Embedding Vector from Scratch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialized LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
