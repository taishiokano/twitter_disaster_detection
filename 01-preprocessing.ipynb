{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01. Preprocessing"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Raw Dataset\n",
    "We download two training dataset from Kaggle\n",
    "\n",
    "- [NLP with Disaster Tweets](https://www.kaggle.com/competitions/nlp-getting-started/data) (NWDT)\n",
    "- [Disasters on social media](https://www.kaggle.com/datasets/jannesklaas/disasters-on-social-media) (DOSM)\n",
    "\n",
    "We use `pandas` for merging two datasets and extracing only relevant features and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "NWDT_PATH = \"data/nlp-with-disaster-tweets-train.csv\"\n",
    "DOSM_PATH = \"data/disasters-on-social-media.csv\"\n",
    "\n",
    "nwdt = pd.read_csv(NWDT_PATH)\n",
    "dosm = pd.read_csv(DOSM_PATH)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modify the DOSM dataset to match the NWDT dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about #earthquake is different cities, s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond, geese are...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  keyword location                                               text  target\n",
       "0     NaN      NaN                 Just happened a terrible car crash       1\n",
       "1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...       1\n",
       "2     NaN      NaN  Heard about #earthquake is different cities, s...       1\n",
       "3     NaN      NaN  there is a forest fire at spot pond, geese are...       1\n",
       "4     NaN      NaN             Forest fire near La Ronge Sask. Canada       1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dosm.loc[dosm[\"choose_one\"] == \"Relevant\", \"target\"] = 1\n",
    "dosm.loc[dosm[\"choose_one\"] == \"Not Relevant\", \"target\"] = 0\n",
    "dosm = dosm.dropna(subset=[\"target\"])\n",
    "dosm[\"target\"] = dosm[\"target\"].astype(\"int\")\n",
    "dosm = dosm[[\"keyword\", \"location\", \"text\", \"target\"]]\n",
    "\n",
    "dosm.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge two datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18468</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18469</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Police investigating after an e-bike collided ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18470</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>The Latest: More Homes Razed by Northern Calif...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18471</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MEG issues Hazardous Weather Outlook (HWO) htt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18472</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#CityofCalgary has activated its Municipal Eme...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18473 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      keyword location                                               text  \\\n",
       "0         NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1         NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2         NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3         NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4         NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "...       ...      ...                                                ...   \n",
       "18468     NaN      NaN  M1.94 [01:04 UTC]?5km S of Volcano Hawaii. htt...   \n",
       "18469     NaN      NaN  Police investigating after an e-bike collided ...   \n",
       "18470     NaN      NaN  The Latest: More Homes Razed by Northern Calif...   \n",
       "18471     NaN      NaN  MEG issues Hazardous Weather Outlook (HWO) htt...   \n",
       "18472     NaN      NaN  #CityofCalgary has activated its Municipal Eme...   \n",
       "\n",
       "       target  \n",
       "0           1  \n",
       "1           1  \n",
       "2           1  \n",
       "3           1  \n",
       "4           1  \n",
       "...       ...  \n",
       "18468       1  \n",
       "18469       1  \n",
       "18470       1  \n",
       "18471       1  \n",
       "18472       1  \n",
       "\n",
       "[18473 rows x 4 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_tweets = pd.concat([nwdt, dosm], ignore_index=True).drop(\"id\", axis=1)\n",
    "train_tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Export the final dataset (`train_tweets`)\n",
    "\n",
    "Note that we use `|` as a seperator for reducing the chance of error, and do not using quoting(leaving `quoting` as `None`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.to_csv(\"data/train-tweets.csv\", sep=\"|\", quoting=None)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenization and Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To clean up our tweet, we use `nltk` library, and implement a series of cleaning process\n",
    "\n",
    "1. We transform the entire tweet into a lower case and tokenize the tweet into each word by using the `word_tokenize` function from the `nltk.tokenize` package. \n",
    "2. We are interested in an only word that is an alphanumeric character.\n",
    "3. We do not use any stop words in English. To consider which word is not a stop word, `nltk.corpus` gives us a list of `stopwords` that we can use to determine it. \n",
    "4. We lemmatize and transform a different form of words into a single baseline form of word i.e. books -> book, children -> child, went / gone -> go. We leverage the `WordNetLemmatizer` for doing this task. \n",
    "\n",
    "Putting it all together, we create a text pre-processing function `text_pre_processed()` for cleaning our texts, and we will use this for cleaning our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def text_pre_processed(text_row):\n",
    "    \"\"\"\n",
    "    Pre-processed text\n",
    "    Input:\n",
    "        text_row (str): a text\n",
    "    \"\"\"\n",
    "    \n",
    "    tokens = word_tokenize(text_row.lower())\n",
    "    words = []\n",
    "    word_net_lemmatizer = WordNetLemmatizer()\n",
    "    for token in tokens:\n",
    "        if token.isalpha() and token not in stopwords.words(\"english\"):\n",
    "            word = word_net_lemmatizer.lemmatize(token)\n",
    "            words.append(word)\n",
    "    \n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets[\"tokenized_clean_text\"] = train_tweets.apply(\n",
    "    lambda row: text_pre_processed(row[\"text\"]),\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the old (pre-cleaned) text to the cleaned text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "old text:  Our Deeds are the Reason of this #earthquake May ALLAH Forgive us all\n",
      "cleaned text:  deed reason earthquake may allah forgive u \n",
      "\n",
      "old text:  Forest fire near La Ronge Sask. Canada\n",
      "cleaned text:  forest fire near la ronge sask canada \n",
      "\n",
      "old text:  All residents asked to 'shelter in place' are being notified by officers. No other evacuation or shelter in place orders are expected\n",
      "cleaned text:  resident asked place notified officer evacuation shelter place order expected \n",
      "\n",
      "old text:  13,000 people receive #wildfires evacuation orders in California \n",
      "cleaned text:  people receive wildfire evacuation order california \n",
      "\n",
      "old text:  Just got sent this photo from Ruby #Alaska as smoke from #wildfires pours into a school \n",
      "cleaned text:  got sent photo ruby alaska smoke wildfire pours school \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    cleaned = \" \".join(train_tweets[\"tokenized_clean_text\"][index])\n",
    "    print(\"old text: \", train_tweets[\"text\"][index])\n",
    "    print(\"cleaned text: \", cleaned, \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, export our cleaned data for doing more cool things such as data exploration and deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tweets.to_csv(\"data/cleaned-tokenized-train-tweets.csv\", sep=\"|\", quoting=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
